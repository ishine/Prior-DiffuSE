train:
  batch_size: 6
#  batch_size: 12
  n_epochs: 50
  loss: "com_mse_loss"
  chunk_length: 48000
  win_size: 320
  fft_num: 320
  win_shift: 160
  feat_type: "sqrt"  # normal sqrt cubic log_1x, or something else not in recommended list (maybe 'none')
  pesq_loss: False
  lam: 1

model:
  name: 'DiffUNet'
model_ddpm:
  name: 'DiffWave'

optim:
  optimizer: 'Adam'
#  lr: 0.001
  lr: 0.0005
  l2: 0.0000001
  half_lr: 3  # whether to decay learning rate to half scale.
  early_stop: 5   # early stop training when no improvement in k epochs.

optim_ddpm:
  optimizer: 'Adam'
  lr: 0.0002
#  lr: 0.0001
  l2: 0.0000001
  half_lr: 3  # whether to decay learning rate to half scale.
  early_stop: 5   # early stop training when no improvement in k epochs.
